{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO+A3S4d15E8/pGLf3gQkeQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -qU typing langchain-community langgraph \"langchain-nvidia-ai-endpoints\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lDx8Ej9O8iqi","executionInfo":{"status":"ok","timestamp":1749485282127,"user_tz":-330,"elapsed":8849,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}},"outputId":"9541ea0b-3edb-416c-d910-2073b5b36d52"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","key = userdata.get('llm_projects')"],"metadata":{"id":"ls6xpNID9Ezw","executionInfo":{"status":"ok","timestamp":1749485284567,"user_tz":-330,"elapsed":670,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from langchain_nvidia_ai_endpoints import ChatNVIDIA\n","from typing import List, Union, TypedDict, Annotated, Sequence\n","from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage, SystemMessage\n","from langgraph.graph import StateGraph, START, END\n","from langgraph.graph.message import add_messages\n","from langgraph.prebuilt import ToolNode\n","from langchain_core.tools import tool\n","\n","llm  = ChatNVIDIA(\n","    model =\"meta/llama3-70b-instruct\" ,\n","    api_key = key,\n","    temperature = 0.7,\n","    top_p = 1,\n","    max_tokens = 1024)"],"metadata":{"id":"4t6EQCKEzVd8","executionInfo":{"status":"ok","timestamp":1749485291709,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Agent I : Agent ChatBot with memory"],"metadata":{"id":"F-H4YgLOZ-KC"}},{"cell_type":"code","source":["class AgentState(TypedDict):\n","    messages : List[Union[HumanMessage,AIMessage]]\n","\n","def process(state:AgentState) -> AgentState:\n","  response = llm.invoke(state[\"messages\"])\n","  state[\"messages\"].append(AIMessage(content=response.content))\n","  print(\"Current state\", state[\"messages\"])\n","  return state\n","\n","graph = StateGraph(AgentState)\n","graph.add_node(\"process\", process)\n","graph.add_edge(START, \"process\")\n","graph.add_edge(\"process\", END)\n","agent = graph.compile()\n","\n","conversation_history = []\n","\n","while (user_input := input(\"Human: \")) != \"exit\":\n","  conversation_history.append(HumanMessage(content=user_input))\n","  agent_response = agent.invoke({\"messages\": conversation_history})\n","  conversation_history = agent_response[\"messages\"]\n","  user_input = input(\"Human: \")\n","\n","with open(\"logging.txt\", \"w\") as file:\n","    file.write(\"Your Conversation Log:\\n\")\n","\n","    for message in conversation_history:\n","        if isinstance(message, HumanMessage):\n","            file.write(f\"You: {message.content}\\n\")\n","        elif isinstance(message, AIMessage):\n","            file.write(f\"AI: {message.content}\\n\\n\")\n","    file.write(\"End of Conversation\")\n","\n","print(\"Conversation saved to logging.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyvi75HY9Q7B","executionInfo":{"status":"ok","timestamp":1749485303443,"user_tz":-330,"elapsed":6041,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}},"outputId":"3f855e2f-2caa-4ea6-8c17-26b9b8053f87"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: exit\n","Conversation saved to logging.txt\n"]}]},{"cell_type":"markdown","source":["Agent II: ReAct Agent [Reason and Action Agent]"],"metadata":{"id":"AMLMofre0t6W"}},{"cell_type":"code","source":["class AgentState(TypedDict):\n","  message : Annotated[Sequence[BaseMessage],add_messages]\n","\n","\n","\n","@tool\n","def add(a:int,b:int):\n","  \"\"\"this is an addition function that adds two numbers\"\"\"\n","  return a + b\n","\n","@tool\n","def substract(a:int,b:int):\n","  \"\"\"this is an subtraction function that subtracts two numbers\"\"\"\n","  return a - b\n","\n","@tool\n","def multiply(a:int,b:int):\n","  \"\"\"this is an multiplication function that multiplies two numbers\"\"\"\n","  return a * b\n","\n","tools = [add,substract,multiply]\n","\n","llm = ChatNVIDIA(\n","    model =\"meta/llama3-70b-instruct\" ,\n","    api_key = key,\n","    temperature = 0.7,\n","    top_p = 1,\n","    max_tokens = 1024).bind_tools(tools)\n","\n","class AgentState(TypedDict):\n","  message : Annotated[Sequence[BaseMessage],add_messages]\n","\n","def model_call(state: AgentState) -> AgentState:\n","  system_prompt = SystemMessage(\n","      content=\"You are an AI asistent, please answer my query to the best of your ability.\")\n","  messages_to_send = [system_prompt] + state[\"message\"]\n","  response = llm.invoke(messages_to_send)\n","  return {\"message\": [response]}\n","\n","def should_continue(state: AgentState) -> AgentState:\n","  last_message = state[\"message\"][-1]\n","  if not last_message.tool_calls:\n","    return \"end\"\n","  else:\n","    return \"continue\"\n","\n","graph = StateGraph(AgentState)\n","graph.add_node(\"model_call\", model_call)\n","\n","tool_node = ToolNode(tools=tools)\n","graph.add_node(\"tool_node\", tool_node)\n","\n","graph.add_edge(START, \"model_call\")\n","\n","graph.add_conditional_edges(\n","    \"model_call\",\n","    should_continue,\n","    {\n","        \"continue\": \"tool_node\",\n","        \"end\": END,\n","    }\n",")\n","\n","graph.add_edge(\"tool_node\", \"model_call\")\n","\n","agent = graph.compile()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbDgTSl3-XjY","executionInfo":{"status":"ok","timestamp":1749485412469,"user_tz":-330,"elapsed":29,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}},"outputId":"39f1ad22-a8bd-4908-d8aa-bb3bd2492b1b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/chat_models.py:592: UserWarning: Model 'meta/llama3-70b-instruct' is not known to support tools. Your tool binding may fail at inference time.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def print_stream(stream):\n","    for s in stream:\n","        message = s[\"message\"][-1]\n","        if isinstance(message, tuple):\n","            print(message)\n","        else:\n","            message.pretty_print()\n","\n","inputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\")]}\n","print_stream(agent.stream(inputs, stream_mode=\"values\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI5lvEvj1Efb","executionInfo":{"status":"ok","timestamp":1749485414408,"user_tz":-330,"elapsed":838,"user":{"displayName":"Aayush Jain","userId":"17987197316438352443"}},"outputId":"6689dfa0-715a-48d7-ceef-3dd24db5ed2e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================\u001b[1m Ai Message \u001b[0m==================================\n","\n","I'll do my best to assist you. Please go ahead and ask your question or share your query, and I'll respond with a helpful and accurate answer. I'm here to help!\n"]}]},{"cell_type":"markdown","source":["Agent III : Drafter"],"metadata":{"id":"U8DkH0kW-Jqc"}},{"cell_type":"code","source":[],"metadata":{"id":"aP3zvsb86MYO"},"execution_count":null,"outputs":[]}]}